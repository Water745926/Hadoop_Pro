#！/bin/bash
#【1】脚本说明：定时清理hdfs文件，包括yarn和spark的临时日志文件、程序执行jar包，hive数据文件(所属：ec2-user)等

#【2】脚本逻辑：如果进入安全模式，先执行相关命令，退出安全模式；
#               为了确保退出命令生效，执行休眠10s之后开始清理。

#【3】执行时间：每周三23:50执行，确保周四打标签逻辑不受hdfs空间不足的影响；
#               每周日23:50执行，确保周一生成头条数据不受hdfs空间不足的影响。
#               每周一、每周五也会执行，23：50分。

#【4】调用路径：crontab
#               50 15 * * 0,1,3,5
#               crontab 历史记录 cd /var/log
#               tail -100 cron

#接收和定义参数
hdfs_status=`hdfs dfsadmin -safemode get | cut -d' ' -f4`

echo --------------------hdfs当前容量如下--------------------
hdfs dfsadmin -report
echo --------------------------------------------------------
echo ----------------------开始执行清理----------------------
if [[ ${hdfs_status} = OFF ]]; then
#       清理yarn日志,清理spark日志,清理spark任务临时jar包路径,清理hive数据量大于1G的表
        hdfs  dfs -rm -r yarn日志目录
        hdfs  dfs -rm -r spark临时jar包目录
        hdfs dfs -rm -r spark日志目录
        for table_name in $(hdfs dfs -du -h hive映射的某一个路径 | grep ' G'|cut -d'G' -f2);do
            echo 清理hive表${table_name}下的数据
            hdfs dfs -rm -r ${table_name}
        done

else
#       hdfs已经进入安全模式，切换用户
        sudo su -
        su hdfs
        hadoop dfsadmin -safemode leave
#       等待安全模式退出生效
        sleep 10s
        exit
#       返回用户（非root用户）用户权限
        exit 
#       清理yarn日志,清理spark日志,清理spark任务临时jar包路径,清理hive数据量大于1G的表
        hdfs  dfs -rm -r yarn日志目录
        hdfs  dfs -rm -r spark临时jar包目录
        hdfs dfs -rm -r spark日志目录
        for table_name in $(hdfs dfs -du -h hive映射的某一个路径 | grep ' G'|cut -d'G' -f2);do
            echo 清理hive表${table_name}下的数据
            hdfs dfs -rm -r ${table_name}
        done
fi
sleep 60s
echo --------------------hdfs清理执行结束，当前容量如下--------------------
hdfs dfsadmin -report
echo --------------------------------------------------------
echo $(date +%F%n%T) 自动化清理一次 >> 日志输出路径log1_clean_hdfs.log
